{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab40d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5aa46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c19c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b734cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewarder(nn.Module):\n",
    "    def __init__(self, num_hidden_units, input_dim):\n",
    "        super(Rewarder,self).__init__()\n",
    "        \n",
    "        self.rewarder_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_units,1)\n",
    "        ) \n",
    "        \n",
    "    def forward(self, input_obs):\n",
    "        return int(self.rewarder_net(input_obs))\n",
    "    \n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_actions, num_hidden_units, input_dim):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        self.actor_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_units,num_actions),\n",
    "            nn.Softmax(dim=0)\n",
    "        ) \n",
    "        \n",
    "        \n",
    "    def forward(self, input_obs):\n",
    "        return self.actor_net(input_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee9c7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.state = self.init_state()\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def init_state(self):\n",
    "        xs = random.sample(range(0,7),5)\n",
    "        ys = random.sample(range(0,7),5)\n",
    "        piece_pos = [xs[0], ys[0]]\n",
    "        players_pos = [xs[4], ys[4]]\n",
    "        positions = list(np.array([players_pos, piece_pos]).flatten())\n",
    "        return positions\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = self.init_state()\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if action == 0: #up\n",
    "            self.state[1] += 1\n",
    "            \n",
    "        if action == 1: #right\n",
    "            self.state[0] += 1\n",
    "            \n",
    "        if action == 2: #down\n",
    "            self.state[1] -= 1\n",
    "            \n",
    "        if action == 3: #left\n",
    "            self.state[0] -= 1\n",
    "            \n",
    "        if (self.step_count >= 25) : # loss max step\n",
    "            self.reset()\n",
    "            # reward function\n",
    "            # with torch.no_grad():\n",
    "            #    reward = int(rewarder(torch.unsqueeze(self.state, 0), training=False)[0])\n",
    "            reward = int(rewarder(self.state))\n",
    "            playing = False\n",
    "            \n",
    "            return reward, playing\n",
    "        \n",
    "        if (self.state[0] == self.state[2]) and (self.state[1] == self.state[3]): # win the game\n",
    "            playing = False\n",
    "            reward = 100\n",
    "            # reward learning model\n",
    "            # with torch.no_grad():\n",
    "            #    reward = int(rewarder(torch.unsqueeze(self.state, 0), training=False)[0])\n",
    "            self.reset()\n",
    "            return reward, playing\n",
    "        \n",
    "        playing = True\n",
    "\n",
    "        # reward function\n",
    "        #with torch.no_grad():\n",
    "        #    reward = int(rewarder(torch.unsqueeze(self.state, 0), training=False)[0])\n",
    "        reward = int(rewarder(torch.Tensor(self.state)))\n",
    "        \n",
    "        return reward, playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "378ea735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 4, 1]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewarder = Rewarder(num_hidden_units = 64, input_dim = 4)\n",
    "agent = Actor(num_actions = 4, num_hidden_units = 64, input_dim = 4)\n",
    "env = GridWorld()\n",
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1125aea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_proba = agent(torch.Tensor(env.state))\n",
    "action_index = torch.argmax(action_proba)\n",
    "reward, playing = env.step(action_index)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "099f3168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 1, 4, 1]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d370f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare (transition_ids, states, actions):\n",
    "    states = 0\n",
    "    d1xs = [states[transition_ids[0]][0][0], states[transition_ids[0]][0][2]]\n",
    "    d1ys = [states[transition_ids[0]][0][1], states[transition_ids[0]][0][3]]\n",
    "    d2xs = [states[transition_ids[1]][0][0], states[transition_ids[0]][0][2]]\n",
    "    d2ys = [states[transition_ids[1]][0][1], states[transition_ids[0]][0][3]]\n",
    "\n",
    "    fig, ax = plt.subplots(2,2)\n",
    "    colors = ['yellow','green']\n",
    "    color_indices = [0, 1, ]\n",
    "    colormap = matplotlib.colors.ListedColormap(colors)\n",
    "    ax[0,0].scatter(d1xs, d1ys, c=color_indices, cmap=colormap)\n",
    "    ax[0,0].set_title(str(decode_action(actions[transition_ids[0]].numpy())))\n",
    "    ax[0,1].scatter(d2xs, d2ys, c=color_indices, cmap=colormap)\n",
    "    ax[0,1].set_title(str(decode_action(actions[transition_ids[1]].numpy())))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f66b044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action(action):\n",
    "    if action == 0:\n",
    "        return 'up'\n",
    "    if action == 1:\n",
    "        return 'right'\n",
    "    if action == 2:\n",
    "        return 'down'\n",
    "    if action == 3:\n",
    "        return 'left'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_update( states, actions, rewarder):\n",
    "    transition_ids = random.sample(range(0, len(states) -2), 2)\n",
    "    compare(transition_ids, states, actions)\n",
    "    pref = input (' select preference a:left, d:right, s:same ')\n",
    "    if pref == 'a':\n",
    "        dist = [ 1, 0 ]\n",
    "    if pref == 'd':\n",
    "        dist = [ 0, 1 ]\n",
    "    if pref == 's':\n",
    "        dist = [ 1, 1 ]\n",
    "        \n",
    "    # stuff gradient\n",
    "    reward_1 = rewarder( state[transition_ids[0] + 1])\n",
    "    reward_2 = rewarder( state[transition_ids[1] + 1])\n",
    "    p1 = torch.exp(reward_1) / (torch.exp(reward_1) + torch.exp(reward_2))\n",
    "    p2 = torch.exp(reward_2) / (torch.exp(reward_1) + torch.exp(reward_2))\n",
    "    loss = - ( p1*dist[0] + p2*dist[1] )\n",
    "    #grads = torch gradient (loss, rewarder variable)\n",
    "    # optimizer ( grads, rewarder)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the reward funtion on a state and action\n",
    "def step_episode(env, model):\n",
    "    env.reset()\n",
    "    action_prob_list = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    playing = True\n",
    "    \n",
    "    while playing == True\n",
    "        observation = expand dim (env.state)\n",
    "        \n",
    "        # run model\n",
    "        action_logits = agent(obs)\n",
    "        \n",
    "        #categorical probabilistic action idx selection\n",
    "        selected_action_idx = random categorical (action_logits, 1 )[ 0, 0]\n",
    "        \n",
    "        states.append(observation)\n",
    "        action.append(selected_action_idx)\n",
    "        \n",
    "        reward, playing = env.step(selected_action_idx)\n",
    "        \n",
    "        #normalize proba\n",
    "        action_probs = softmax function ( action_logits)\n",
    "        probability_of_taking_selected_action = action_probs[0, selected_action_idx]\n",
    "        action_probs_list.append(probability_of_taking_selected_action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    return action_probs_list, rewards, states, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Actor (num_action = 4, num_hidden_units = 100)\n",
    "\n",
    "rewarder = Rewarder(num_hidden_unites = 100)\n",
    "\n",
    "env = GridWorld()\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# loss = self.criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "#                acc_loss = acc_loss + loss.item()\n",
    "#                self.autoenc.zero_grad()\n",
    "#                loss.backward()\n",
    "#                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_g(reward_trajectory, gamma):\n",
    "    ez_discount = np.array([ gamma**n for n in range(len(reward_trajectory))])\n",
    "    gs = []\n",
    "    reward_trajectory = np.array(reward_trajectory)\n",
    "    for ts in range(len(reward_trajectory)):\n",
    "        to_end_rewards = reward_trajectory[ts:]\n",
    "        eq_len_discount = ez_discount[:len(reward_trajectory[ts:])]\n",
    "        total_value = np.multiply(to_end_rewards, eq_len_discount)\n",
    "        g = sum(total_value)\n",
    "        gs.append(g)\n",
    "    return gs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c85037",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(5000):\n",
    "    action_probs, rewards, states ,actions = step_episode (env, agent)\n",
    "    loss = actor_loss (action_probs ,rewards)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
