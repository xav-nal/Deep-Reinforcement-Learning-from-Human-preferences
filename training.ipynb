{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ff9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c876196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad9c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc699f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewarder(nn.Module):\n",
    "    def __init__(self, num_hidden_units, input_dim)\n",
    "        super(Rewarder,self).__init__()\n",
    "        \n",
    "        self.rewarder_net == nn.Sequential(\n",
    "            nn.Linear(input_dim, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden,1)\n",
    "        ) \n",
    "        \n",
    "    def call (self, input_obs):\n",
    "        return self.reward_layer(input_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f5a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.state = self.init_state()\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def init_state(self):\n",
    "        xs = random.sample(range(0,7),5)\n",
    "        ys = random.sample(range(0,7),5)\n",
    "        piece_pos = [xs[0], ys[0]]\n",
    "        players_pos = [xs[4], ys[4]]\n",
    "        positions = list(np.array([players_pos, piece_pos]).flatten())\n",
    "        return positions\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = self.init_state()\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if action == 0: #up\n",
    "            self.state[1] += 1\n",
    "            \n",
    "        if action == 1: #right\n",
    "            self.state[0] += 1\n",
    "            \n",
    "        if action == 2: #down\n",
    "            self.state[1] -= 1\n",
    "            \n",
    "        if action == 3: #left\n",
    "            self.state[0] -= 1\n",
    "            \n",
    "        if (self.step_count >= 25) : # loss max step\n",
    "            self.reset()\n",
    "            # reward function\n",
    "            # with torch.no_grad():\n",
    "            #    reward = int(rewarder(torch.unsqueeze(self.state, 0), training=False)[0])\n",
    "            reward = -100\n",
    "            playing = False\n",
    "            \n",
    "            return reward, playing\n",
    "        \n",
    "        if (self.state[0] == self.state[2]) and (self.state[1] == self.state[3]): # win the game\n",
    "            playing = False\n",
    "            reward = 100\n",
    "            # reward learning model\n",
    "            # with torch.no_grad():\n",
    "            #    reward = int(rewarder(torch.unsqueeze(self.state, 0), training=False)[0])\n",
    "            self.reset()\n",
    "            return reward, playing\n",
    "        \n",
    "        playing = True\n",
    "\n",
    "        # reward function\n",
    "        #with torch.no_grad():\n",
    "        #    reward = int(rewarder(torch.unsqueeze(self.state, 0), training=False)[0])\n",
    "        reward = 0\n",
    "        \n",
    "        return reward, playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c5958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d814cd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 3, 1, 6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d6f782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06abc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 1, 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7401831a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c30ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 1, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55eb5c28",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Actor' object has no attribute 'actor_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mActor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hidden_units\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m, in \u001b[0;36mActor.__init__\u001b[1;34m(self, num_actions, num_hidden_units, input_dim)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_actions, num_hidden_units, input_dim):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Actor,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_net\u001b[49m \u001b[38;5;241m==\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      6\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(num_hidden),\n\u001b[0;32m      7\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m      8\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(num_hidden,num_actions)\n\u001b[0;32m      9\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Actor' object has no attribute 'actor_net'"
     ]
    }
   ],
   "source": [
    "agent = Actor(num_actions = 4, num_hidden_units = 64, input_dim = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5026afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_actions, num_hidden_units, input_dim):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        self.actor_net == nn.Sequential(\n",
    "            nn.Linear(num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden,num_actions)\n",
    "        ) \n",
    "        \n",
    "        \n",
    "    def forward(self, input_obs):\n",
    "        return self.actor_net(input_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "163d0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "reward, playing = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3543e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare (transition_ids, states, actions):\n",
    "    states = 0\n",
    "    d1xs = [states[transition_ids[0]][0][0], states[transition_ids[0]][0][2]]\n",
    "    d1ys = [states[transition_ids[0]][0][1], states[transition_ids[0]][0][3]]\n",
    "    d2xs = [states[transition_ids[1]][0][0], states[transition_ids[0]][0][2]]\n",
    "    d2ys = [states[transition_ids[1]][0][1], states[transition_ids[0]][0][3]]\n",
    "\n",
    "    fig, ax = plt.subplots(2,2)\n",
    "    colors = ['yellow','green']\n",
    "    color_indices = [0, 1, ]\n",
    "    colormap = matplotlib.colors.ListedColormap(colors)\n",
    "    ax[0,0].scatter(d1xs, d1ys, c=color_indices, cmap=colormap)\n",
    "    ax[0,0].set_title(str(decode_action(actions[transition_ids[0]].numpy())))\n",
    "    ax[0,1].scatter(d2xs, d2ys, c=color_indices, cmap=colormap)\n",
    "    ax[0,1].set_title(str(decode_action(actions[transition_ids[1]].numpy())))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71c8c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action(action):\n",
    "    if action == 0:\n",
    "        return 'up'\n",
    "    if action == 1:\n",
    "        return 'right'\n",
    "    if action == 2:\n",
    "        return 'down'\n",
    "    if action == 3:\n",
    "        return 'left'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_update( states, actions, rewarder):\n",
    "    transition_ids = random.sample(range(0, len(states) -2), 2)\n",
    "    compare(transition_ids, states, actions)\n",
    "    pref = input (' select preference a:left, d:right, s:same ')\n",
    "    if pref == 'a':\n",
    "        dist = [ 1, 0 ]\n",
    "    if pref == 'd':\n",
    "        dist = [ 0, 1 ]\n",
    "    if pref == 's':\n",
    "        dist = [ 1, 1 ]\n",
    "        \n",
    "    # stuff gradient\n",
    "    reward_1 = rewarder( state[transition_ids[0] + 1])\n",
    "    reward_2 = rewarder( state[transition_ids[1] + 1])\n",
    "    p1 = torch.exp(reward_1) / (torch.exp(reward_1) + torch.exp(reward_2))\n",
    "    p2 = torch.exp(reward_2) / (torch.exp(reward_1) + torch.exp(reward_2))\n",
    "    loss = - ( p1*dist[0] + p2*dist[1] )\n",
    "    #grads = torch gradient (loss, rewarder variable)\n",
    "    # optimizer ( grads, rewarder)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4325acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the reward funtion on a state and action\n",
    "def step_episode(env, model):\n",
    "    env.reset()\n",
    "    action_prob_list = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    playing = True\n",
    "    \n",
    "    while playing == True\n",
    "        observation = expand dim (env.state)\n",
    "        \n",
    "        # run model\n",
    "        action_logits = agent(obs)\n",
    "        \n",
    "        #categorical probabilistic action idx selection\n",
    "        selected_action_idx = random categorical (action_logits, 1 )[ 0, 0]\n",
    "        \n",
    "        states.append(observation)\n",
    "        action.append(selected_action_idx)\n",
    "        \n",
    "        reward, playing = env.step(selected_action_idx)\n",
    "        \n",
    "        #normalize proba\n",
    "        action_probs = softmax function ( action_logits)\n",
    "        probability_of_taking_selected_action = action_probs[0, selected_action_idx]\n",
    "        action_probs_list.append(probability_of_taking_selected_action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    return action_probs_list, rewards, states, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Actor (num_action = 4, num_hidden_units = 100)\n",
    "\n",
    "rewarder = Rewarder(num_hidden_unites = 100)\n",
    "\n",
    "env = GridWorld()\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# loss = self.criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "#                acc_loss = acc_loss + loss.item()\n",
    "#                self.autoenc.zero_grad()\n",
    "#                loss.backward()\n",
    "#                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a7b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_g(reward_trajectory, gamma):\n",
    "    ez_discount = np.array([ gamma**n for n in range(len(reward_trajectory))])\n",
    "    gs = []\n",
    "    reward_trajectory = np.array(reward_trajectory)\n",
    "    for ts in range(len(reward_trajectory)):\n",
    "        to_end_rewards = reward_trajectory[ts:]\n",
    "        eq_len_discount = ez_discount[:len(reward_trajectory[ts:])]\n",
    "        total_value = np.multiply(to_end_rewards, eq_len_discount)\n",
    "        g = sum(total_value)\n",
    "        gs.append(g)\n",
    "    return gs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(5000):\n",
    "    action_probs, rewards, states ,actions = step_episode (env, agent)\n",
    "    loss = actor_loss (action_probs ,rewards)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
